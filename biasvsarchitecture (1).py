# -*- coding: utf-8 -*-
"""BiasVsArchitecture.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15CtqjMuMlPSgQmaKHpe2IPvoRoiE4NCV
"""



!pip install transformers torch pandas numpy scipy accelerate -q

import torch
import pandas as pd
import numpy as np
# Import both AutoModel and AutoModelForCausalLM
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from scipy.spatial.distance import euclidean
import gc

print("\n--- Initializing setup ---")

# --- Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DATA_FILE_NAME = "crows_pairs_anonymized.csv"
print(f"Using device: {device}")

MODELS_TO_RUN = [
    "albert-base-v2",
    "distilbert-base-uncased",
    "bert-base-uncased",
    "bert-large-uncased",
    "roberta-base",
    "roberta-large",
    "distilroberta-base",
    "openai-gpt",
    "gpt2"
]

DECODER_MODELS = ["openai-gpt", "gpt2"]

try:
    df = pd.read_csv("crows_pairs_anonymized.csv")
    print("File loaded successfully.")
except FileNotFoundError:
    print(f"ERROR: Please upload '{DATA_FILE_NAME}' to your Colab session first.")
    assert False, "File not found."

def find_distinct_words(sent1, sent2):
    words1 = set(str(sent1).lower().replace('.', '').replace(',', '').split())
    words2 = set(str(sent2).lower().replace('.', '').replace(',', '').split())
    distinct_words = list(words1.symmetric_difference(words2))
    word1 = next((w for w in distinct_words if w in words1), None)
    word2 = next((w for w in distinct_words if w in words2), None)
    return word1, word2

def get_model_outputs(sentence, target_word, model, tokenizer):
    inputs = tokenizer(str(sentence), return_tensors="pt", truncation=True, max_length=128).to(device)
    with torch.no_grad():
        outputs = model(**inputs, output_attentions=True)
    if outputs.attentions is None or any(att is None for att in outputs.attentions):
        print(f"    - Warning: Model '{model.config._name_or_path}' did not return valid attention weights.")
        return None
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
    try:
        word_indices = [i for i, token in enumerate(tokens) if target_word.lower() in token.lower()]
        target_index = word_indices[0]
    except IndexError:
        return None
    layer_attentions = [torch.sum(torch.mean(layer_attention, dim=1).squeeze(0)[:, target_index]).item() for layer_attention in outputs.attentions]
    return layer_attentions

all_results = []
# To run on the full dataset, change 'df.head(5)' to 'df'.
sample_df = df

for model_name in MODELS_TO_RUN:
    print(f"\n--- Processing Model: {model_name} ---")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        if model_name in DECODER_MODELS:
            model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(device)
        else:
            model = AutoModel.from_pretrained(model_name, trust_remote_code=True, attn_implementation="eager").to(device)

        model.eval()

        for index, row in sample_df.iterrows():
            biased_sent = row['sent_more']
            neutral_sent = row['sent_less']
            word_b, word_n = find_distinct_words(biased_sent, neutral_sent)
            if not word_b or not word_n: continue
            att_b = get_model_outputs(biased_sent, word_b, model, tokenizer)
            att_n = get_model_outputs(neutral_sent, word_n, model, tokenizer)
            if att_b is None or att_n is None: continue
            sad_scores = [abs(att_b[i] - att_n[i]) for i in range(len(att_b))]
            all_results.append({
                'model_name': model_name, 'pair_index': index,
                'max_sad': np.max(sad_scores), 'max_sad_layer': np.argmax(sad_scores) + 1,
            })
            print(f"  Processed pair {index} for {model_name}")

    except Exception as e:
        print(f"Could not process model {model_name}. Error: {e}")

    del model
    del tokenizer
    gc.collect()
    torch.cuda.empty_cache()

print("\n--- Overall Analysis Complete! ---")
if all_results:
    results_df = pd.DataFrame(all_results)
    print("\n--- Raw Results (Sample) ---")
    print(results_df.head())
    print("\n--- Mean SAD per Model (Insight for Table 2) ---")
    mean_sad_per_model = results_df.groupby('model_name')['max_sad'].mean().sort_values(ascending=False).reset_index()
    print(mean_sad_per_model)
else:
    print("\nNo results were generated. Please check for errors.")

import matplotlib.pyplot as plt
import seaborn as sns

# Set the style for the plot
plt.style.use('seaborn-v0_8-whitegrid')

# Create the figure and axes for the plot
plt.figure(figsize=(12, 8))

# Create the bar plot using seaborn
# The 'mean_sad_per_model' DataFrame is already sorted from the previous step
barplot = sns.barplot(
    data=mean_sad_per_model,
    x='model_name',
    y='max_sad',
    palette='viridis' # You can choose other color palettes like 'plasma' or 'coolwarm'
)

# Add a title and labels for clarity
plt.title('Average Self-Attention Difference (SAD) Across Models', fontsize=16)
plt.xlabel('Model Name', fontsize=12)
plt.ylabel('Mean of Max SAD Scores', fontsize=12)

# Rotate the x-axis labels to prevent them from overlapping
plt.xticks(rotation=45, ha='right')

# Add the exact value on top of each bar
for p in barplot.patches:
    barplot.annotate(format(p.get_height(), '.2f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha = 'center', va = 'center',
                   xytext = (0, 9),
                   textcoords = 'offset points')

# Ensure everything fits nicely in the saved image
plt.tight_layout()

# Display the plot
plt.show()

from google.colab import files

# Save the DataFrame to a CSV file
results_df.to_csv('full_analysis_results.csv', index=False)

# Download the file to your computer
files.download('full_analysis_results.csv')

sentence_pairs = []

print("\n--- Processing dataset into sentence pairs ---")

for example in ds:
    context = example['context']
    stereotype_sentence = None
    anti_stereotype_sentence = None

    for i, sentence_text in enumerate(example['sentences']['sentence']):
        label = example['sentences']['gold_label'][i]   # 0 = stereotype, 1 = anti-stereotype

        # Case 1: sentence_text is just a word/phrase → insert into BLANK
        if "BLANK" in context and not sentence_text.lower().startswith(context[:15].lower()):
            parts = context.split("BLANK")
            candidate = (parts[0] + sentence_text + parts[1]).strip()
        else:
            # Case 2: already a full sentence → use directly
            candidate = sentence_text.strip()

        # Assign based on label
        if label == 0:
            stereotype_sentence = candidate
        elif label == 1:
            anti_stereotype_sentence = candidate

    if stereotype_sentence and anti_stereotype_sentence:
        sentence_pairs.append({
            "sentence_biased": stereotype_sentence,
            "sentence_neutral": anti_stereotype_sentence
        })

stereoset_df = pd.DataFrame(sentence_pairs)

print("\n--- Data preparation complete! ---")
print(stereoset_df.head())

# ==============================================================================
# STEP 1: INSTALL LIBRARIES
# ==============================================================================
print("--- Installing required libraries ---")
!pip install transformers torch pandas numpy scipy accelerate -q


# ==============================================================================
# STEP 2: IMPORTS & SETUP
# ==============================================================================
import torch
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModel
from scipy.spatial.distance import euclidean
import gc

print("\n--- Initializing setup ---")

# --- Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DATA_FILE_NAME = "crows_pairs_anonymized.csv" # Or use your stereoset_df
print(f"Using device: {device}")

# --- Lists of Models ---
MODELS_TO_RUN = [
    "albert-base-v2", "distilbert-base-uncased", "bert-base-uncased",
    "bert-large-uncased", "roberta-base", "roberta-large", "distilroberta-base",
    "openai-gpt", "gpt2"
]
BERT_BASED_MODELS = [
    "albert-base-v2", "distilbert-base-uncased", "bert-base-uncased",
    "bert-large-uncased", "roberta-base", "roberta-large", "distilroberta-base"
]


# ==============================================================================
# STEP 3: LOAD YOUR DATA
# ==============================================================================
# This section assumes you have either uploaded 'crows_pairs_anonymized.csv'
# OR you have a DataFrame named 'stereoset_df' from the previous step.

# Option 1: Load from CSV
# print(f"\n--- Reading data from {DATA_FILE_NAME} ---")
# try:
#     df = pd.read_csv(DATA_FILE_NAME)
#     print("File loaded successfully.")
# except FileNotFoundError:
#     print(f"ERROR: Please upload '{DATA_FILE_NAME}' to your Colab session first.")
#     assert False, "File not found."

# Option 2: Use the stereoset_df you already created.
# Make sure the data preparation cell has been run successfully.
df = stereoset_df
print("--- Using the prepared stereoset_df DataFrame ---")


# ==============================================================================
# STEP 4: DEFINE ROBUST CORE FUNCTIONS (UPDATED)
# ==============================================================================
def find_distinct_words(sent1, sent2):
    words1 = set(str(sent1).lower().replace('.', '').replace(',', '').split())
    words2 = set(str(sent2).lower().replace('.', '').replace(',', '').split())
    distinct_words = list(words1.symmetric_difference(words2))
    word1 = next((w for w in distinct_words if w in words1), None)
    word2 = next((w for w in distinct_words if w in words2), None)
    return word1, word2

def get_model_outputs(sentence, target_word, model, tokenizer):
    # Initialize return values to None
    layer_attentions = None
    cls_embeddings = None

    try:
        inputs = tokenizer(str(sentence), return_tensors="pt", truncation=True, max_length=128).to(device)
        with torch.no_grad():
            outputs = model(**inputs, output_attentions=True, output_hidden_states=True)

        # --- Defensive Attention Extraction ---
        if hasattr(outputs, 'attentions') and outputs.attentions is not None and all(att is not None for att in outputs.attentions):
            tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
            word_indices = [i for i, token in enumerate(tokens) if target_word.lower() in token.lower()]
            if word_indices:
                target_index = word_indices[0]
                layer_attentions = [torch.sum(torch.mean(layer_attention, dim=1).squeeze(0)[:, target_index]).item() for layer_attention in outputs.attentions]
        else:
            print(f"    - Warning: Model '{model.config._name_or_path}' did not return valid attention weights.")

        # --- Defensive CLS Embedding Extraction ---
        if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
            cls_embeddings = [layer_hidden_state[0, 0, :].cpu().numpy() for layer_hidden_state in outputs.hidden_states[1:]]

    except Exception as e:
        print(f"    - Error during model processing: {e}")
        return None, None # Return a tuple of Nones on any error

    # Always return a tuple of two items
    return layer_attentions, cls_embeddings


# ==============================================================================
# STEP 5: RUN THE MAIN ANALYSIS LOOP (UPDATED)
# ==============================================================================
all_results = []
# Change to 'df' for the full run
sample_df = df

for model_name in MODELS_TO_RUN:
    print(f"\n--- Processing Model: {model_name} ---")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token

        # Simplified model loading using AutoModel for all
        model = AutoModel.from_pretrained(model_name, trust_remote_code=True, attn_implementation="eager").to(device)
        model.eval()

        for index, row in sample_df.iterrows():
            biased_sent = row['sentence_biased']
            neutral_sent = row['sentence_neutral']

            word_b, word_n = find_distinct_words(biased_sent, neutral_sent)
            if not word_b or not word_n: continue

            att_b, cls_b = get_model_outputs(biased_sent, word_b, model, tokenizer)
            att_n, cls_n = get_model_outputs(neutral_sent, word_n, model, tokenizer)

            # Skip if we couldn't get the necessary data
            if att_b is None or att_n is None: continue

            sad_scores = [abs(att_b[i] - att_n[i]) for i in range(len(att_b))]
            result_dict = {
                'model_name': model_name, 'pair_index': index,
                'max_sad': np.max(sad_scores), 'max_sad_layer': np.argmax(sad_scores) + 1,
            }

            if model_name in BERT_BASED_MODELS and cls_b is not None and cls_n is not None:
                cls_distances = [euclidean(cls_b[i], cls_n[i]) for i in range(len(cls_b))]
                result_dict['max_cls_dist'] = np.max(cls_distances)
                result_dict['max_cls_dist_layer'] = np.argmax(cls_distances) + 1

            all_results.append(result_dict)
        print(f"  Finished processing {model_name}.")

    except Exception as e:
        print(f"Could not process model {model_name}. Error: {e}")

    del model, tokenizer
    gc.collect()
    torch.cuda.empty_cache()


# ==============================================================================
# STEP 6: DISPLAY AGGREGATED RESULTS
# ==============================================================================
print("\n--- Overall Analysis Complete! ---")
if all_results:
    results_df = pd.DataFrame(all_results)

    print("\n--- Mean SAD per Model ---")
    mean_sad_per_model = results_df.groupby('model_name')['max_sad'].mean().sort_values(ascending=False).reset_index()
    print(mean_sad_per_model)

    print("\n--- Most Frequent Layer with Max CLS Distance ---")
    cls_results_df = results_df.dropna(subset=['max_cls_dist_layer'])
    if not cls_results_df.empty:
      cls_layer_mode = cls_results_df.groupby('model_name')['max_cls_dist_layer'].apply(lambda x: x.mode()[0]).reset_index()
      print(cls_layer_mode)
    else:
      print("No CLS distance results were generated.")
else:
    print("\nNo results were generated. Please check for errors.")

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# This assumes 'all_results' list was successfully populated by the main analysis script.
if 'all_results' in locals() and all_results:
    results_df_stereoset = pd.DataFrame(all_results)

    # --- 1. AGGREGATE SAD RESULTS ---
    print("--- Mean SAD per Model (StereoSet) ---")
    mean_sad_per_model_stereoset = results_df_stereoset.groupby('model_name')['max_sad'].mean().sort_values(ascending=False).reset_index()
    print(mean_sad_per_model_stereoset)

    # --- 2. AGGREGATE CLS DISTANCE RESULTS ---
    print("\n--- Most Frequent Layer with Max CLS Distance (StereoSet) ---")
    cls_results_df_stereoset = results_df_stereoset.dropna(subset=['max_cls_dist_layer'])
    if not cls_results_df_stereoset.empty:
        cls_layer_mode_stereoset = cls_results_df_stereoset.groupby('model_name')['max_cls_dist_layer'].apply(lambda x: x.mode()[0]).reset_index()
        print(cls_layer_mode_stereoset)
    else:
        print("No CLS distance results were generated.")

    # --- 3. PLOT SAD RESULTS ---
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.figure(figsize=(12, 8))
    sns.barplot(
        data=mean_sad_per_model_stereoset,
        x='model_name',
        y='max_sad',
        palette='viridis'
    )
    plt.title('Average SAD Across Models (StereoSet Data)', fontsize=16)
    plt.xlabel('Model Name', fontsize=12)
    plt.ylabel('Mean of Max SAD Scores', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

    # --- 4. PLOT CLS DISTANCE RESULTS ---
    if not cls_results_df_stereoset.empty:
        plt.figure(figsize=(12, 8))
        sns.barplot(
            data=cls_layer_mode_stereoset.sort_values('max_cls_dist_layer', ascending=False),
            x='model_name',
            y='max_cls_dist_layer',
            palette='plasma'
        )
        plt.title('Most Frequent Layer with Max CLS Distance (StereoSet Data)', fontsize=16)
        plt.xlabel('Model Name', fontsize=12)
        plt.ylabel('Layer Number', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()

else:
    print("Could not find 'all_results'. Please re-run the main analysis script first.")

from google.colab import files

# Save the DataFrame to a CSV file
results_df_stereoset.to_csv('stereoset_analysis_results.csv', index=False)

# Download the file to your computer
files.download('stereoset_analysis_results.csv')

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# ==============================================================================
# STEP 1: DEFINE FILENAMES
# ==============================================================================
# These should be the names of the files you have uploaded to Colab.
CROWS_RESULTS_FILE = 'crows_pairs_analysis_results.csv'
STEREOSET_RESULTS_FILE = 'stereoset_analysis_results.csv'


try:
    # ==============================================================================
    # STEP 2: LOAD DATA FROM CSV FILES
    # ==============================================================================
    print(f"--- Loading data from {CROWS_RESULTS_FILE} and {STEREOSET_RESULTS_FILE} ---")

    # Load the raw results for both datasets
    df_crows = pd.read_csv(CROWS_RESULTS_FILE)
    df_stereoset = pd.read_csv(STEREOSET_RESULTS_FILE)

    # Add a 'dataset' column to each DataFrame to keep track of the source
    df_crows['dataset'] = 'CrowS-Pairs'
    df_stereoset['dataset'] = 'StereoSet'

    # Combine both raw results into a single DataFrame
    results_df = pd.concat([df_crows, df_stereoset], ignore_index=True)
    print("Data loaded successfully.")

    # ==============================================================================
    # STEP 3: GENERATE PLOT FOR SAD SCORE COMPARISON
    # ==============================================================================
    print("\n--- Generating plot for Mean SAD Scores ---")

    plt.style.use('seaborn-v0_8-whitegrid')
    plt.figure(figsize=(15, 8))

    # Seaborn's barplot can calculate the mean directly from the raw data
    sns.barplot(
        data=results_df,
        x='model_name',
        y='max_sad',
        hue='dataset',
        palette='viridis',
        estimator=np.mean
    )

    plt.title('Comparison of Mean SAD Scores Across Datasets', fontsize=18)
    plt.xlabel('Model Name', fontsize=12)
    plt.ylabel('Mean of Max SAD Scores', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.legend(title='Dataset', fontsize=12)
    plt.tight_layout()
    plt.show()


    # ==============================================================================
    # STEP 4: GENERATE PLOT FOR CLS LAYER COMPARISON
    # ==============================================================================
    print("\n--- Generating plot for Max CLS Distance Layer ---")

    # Filter for results that have CLS data and find the most frequent layer (mode)
    cls_results = results_df.dropna(subset=['max_cls_dist_layer'])
    if not cls_results.empty:
        cls_comparison = cls_results.groupby(['dataset', 'model_name'])['max_cls_dist_layer'].apply(lambda x: x.mode()[0]).reset_index()

        plt.figure(figsize=(15, 8))

        sns.barplot(
            data=cls_comparison,
            x='model_name',
            y='max_cls_dist_layer',
            hue='dataset',
            palette='plasma'
        )

        plt.title('Comparison of Most Frequent Max CLS Distance Layer', fontsize=18)
        plt.xlabel('Model Name', fontsize=12)
        plt.ylabel('Layer Number', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.legend(title='Dataset', fontsize=12)
        plt.tight_layout()
        plt.show()
    else:
        print("No CLS distance data found to plot.")

except FileNotFoundError as e:
    print(f"\nError: Could not find a file. {e}")
    print("Please make sure your uploaded CSV files have the exact names specified at the top of the script.")

import pandas as pd
import numpy as np

# ==============================================================================
# STEP 1: DEFINE FILENAMES
# ==============================================================================
# These should be the names of the files you have uploaded to Colab.
CROWS_RESULTS_FILE = 'crows_pairs_analysis_results.csv'
STEREOSET_RESULTS_FILE = 'stereoset_analysis_results.csv'


try:
    # ==============================================================================
    # STEP 2: LOAD AND COMBINE DATA FROM CSV FILES
    # ==============================================================================
    print(f"--- Loading data from {CROWS_RESULTS_FILE} and {STEREOSET_RESULTS_FILE} ---")

    # Load the raw results for both datasets
    df_crows = pd.read_csv(CROWS_RESULTS_FILE)
    df_stereoset = pd.read_csv(STEREOSET_RESULTS_FILE)

    # Add a 'dataset' column to each DataFrame to keep track of the source
    df_crows['dataset'] = 'CrowS-Pairs'
    df_stereoset['dataset'] = 'StereoSet'

    # Combine both raw results into a single DataFrame
    results_df = pd.concat([df_crows, df_stereoset], ignore_index=True)
    print("Data loaded and combined successfully.")

    # ==============================================================================
    # STEP 3: GENERATE TABLE 1 (CLS Layers)
    # ==============================================================================
    print("\n\n--- Generating Table 1: Most Frequent Layer with Max CLS Distance ---")

    # Filter for results that have CLS data
    cls_results = results_df.dropna(subset=['max_cls_dist_layer'])

    if not cls_results.empty:
        # Group by dataset and model, find the most frequent layer (mode), then unstack
        cls_layer_comparison = cls_results.groupby(['dataset', 'model_name'])['max_cls_dist_layer'].apply(lambda x: x.mode().iloc[0]).unstack(level=0)

        # Rename columns to match the paper's format
        cls_layer_comparison = cls_layer_comparison.rename(columns={'CrowS-Pairs': 'Crows-Pairs', 'StereoSet': 'Stereoset'})

        print(cls_layer_comparison)
    else:
        print("No CLS distance results were generated for this table.")


    # ==============================================================================
    # STEP 4: GENERATE TABLE 2 (Mean SAD Scores)
    # ==============================================================================
    print("\n\n--- Generating Table 2: Mean SAD per Model ---")

    # Group by dataset and model, calculate the mean, then unstack to create the table format
    mean_sad_comparison = results_df.groupby(['dataset', 'model_name'])['max_sad'].mean().unstack(level=0)

    # Rename columns for clarity
    mean_sad_comparison = mean_sad_comparison.rename(columns={'CrowS-Pairs': 'SAD-Mean (CrowS-Pairs)', 'StereoSet': 'SAD-Mean (StereoSet)'})

    # Sort by one of the columns for a clear ranking
    if 'SAD-Mean (CrowS-Pairs)' in mean_sad_comparison.columns:
        mean_sad_comparison = mean_sad_comparison.sort_values('SAD-Mean (CrowS-Pairs)', ascending=False)

    print(mean_sad_comparison)


except FileNotFoundError as e:
    print(f"\nError: Could not find a file. {e}")
    print("Please make sure your uploaded CSV files have the exact names specified at the top of the script.")

# ==============================================================================
# STEP 1: INSTALL LIBRARIES
# ==============================================================================
print("--- Installing required libraries ---")
!pip install datasets transformers torch pandas numpy scipy accelerate -q


# ==============================================================================
# STEP 2: IMPORTS & SETUP
# ==============================================================================
import torch
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
import gc
from datasets import load_dataset

print("\n--- Initializing setup ---")

# --- Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- Lists of Models ---
MODELS_TO_RUN = [
    "albert-base-v2", "distilbert-base-uncased", "bert-base-uncased",
    "bert-large-uncased", "roberta-base", "roberta-large", "distilroberta-base",
    "openai-gpt", "gpt2"
]
DECODER_MODELS = ["openai-gpt", "gpt2"]


# ==============================================================================
# STEP 3: DEFINE HELPER FUNCTIONS
# ==============================================================================
def find_distinct_words(sent1, sent2):
    words1 = set(str(sent1).lower().replace('.', '').replace(',', '').split())
    words2 = set(str(sent2).lower().replace('.', '').replace(',', '').split())
    distinct_words = list(words1.symmetric_difference(words2))
    word1 = next((w for w in distinct_words if w in words1), None)
    word2 = next((w for w in distinct_words if w in words2), None)
    return word1, word2

def get_attention_weights(sentence, target_word, model, tokenizer):
    layer_attentions = None
    try:
        inputs = tokenizer(str(sentence), return_tensors="pt", truncation=True, max_length=128).to(device)
        with torch.no_grad():
            outputs = model(**inputs, output_attentions=True)
        if hasattr(outputs, 'attentions') and outputs.attentions is not None and all(att is not None for att in outputs.attentions):
            tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
            word_indices = [i for i, token in enumerate(tokens) if target_word.lower() in token.lower()]
            if word_indices:
                target_index = word_indices[0]
                layer_attentions = [torch.sum(torch.mean(layer_attention, dim=1).squeeze(0)[:, target_index]).item() for layer_attention in outputs.attentions]
    except Exception as e:
        print(f"    - Error during model processing: {e}")
        return None
    return layer_attentions


# ==============================================================================
# STEP 4: RUN ANALYSIS FOR TABLE 3
# ==============================================================================
table3_results = []
datasets_to_run = {
    "CrowS-Pairs": "crows_pairs_anonymized.csv",
    "StereoSet": "McGill-NLP/stereoset"
}

for dataset_name, data_source in datasets_to_run.items():
    print(f"\n{'='*20}\n--- Starting Analysis for: {dataset_name} ---\n{'='*20}")

    # --- Load and Prepare Data ---
    if dataset_name == "CrowS-Pairs":
        try:
            df = pd.read_csv("crows_pairs_anonymized.csv")
            df = df.rename(columns={'sent_more': 'sentence_biased', 'sent_less': 'sentence_neutral'})
        except FileNotFoundError:
            print(f"ERROR: Please upload '{data_source}' to run the analysis.")
            continue
    elif dataset_name == "StereoSet":
        ds = load_dataset(data_source, "intrasentence")['validation']
        sentence_pairs = []
        for example in ds:
            context, s_stereotype, s_anti = example['context'], None, None
            for i in range(len(example['sentences']['sentence'])):
                label, text = example['sentences']['gold_label'][i], example['sentences']['sentence'][i]
                if "BLANK" in context:
                    if label == 0: s_stereotype = context.split('BLANK')[0] + text + context.split('BLANK')[1]
                    elif label == 1: s_anti = context.split('BLANK')[0] + text + context.split('BLANK')[1]
            if s_stereotype and s_anti:
                sentence_pairs.append({'sentence_biased': s_stereotype, 'sentence_neutral': s_anti})
        df = pd.DataFrame(sentence_pairs)

    # --- Run Models on the Loaded Data ---
    for model_name in MODELS_TO_RUN:
        print(f"\n--- Processing Model: {model_name} on {dataset_name} ---")
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token

            model_class = AutoModelForCausalLM if model_name in DECODER_MODELS else AutoModel
            model = model_class.from_pretrained(model_name, attn_implementation="eager").to(device)
            model.eval()

            # Using df.head(20) for a quicker demonstration. Remove .head(20) for the full run.
            for index, row in df.head(20).iterrows():
                word_b, word_n = find_distinct_words(row['sentence_biased'], row['sentence_neutral'])
                if not word_b or not word_n: continue

                att_b = get_attention_weights(row['sentence_biased'], word_b, model, tokenizer)
                att_n = get_attention_weights(row['sentence_neutral'], word_n, model, tokenizer)

                if att_b is None or att_n is None: continue

                # Find the layer with the highest attention for each sentence
                layer_max_att_biased = np.argmax(att_b) + 1
                layer_max_att_neutral = np.argmax(att_n) + 1

                table3_results.append({
                    'dataset': dataset_name,
                    'model_name': model_name,
                    'biased_max_layer': layer_max_att_biased,
                    'neutral_max_layer': layer_max_att_neutral,
                })
            print(f"  Finished processing {model_name}.")

        except Exception as e:
            print(f"Could not process model {model_name}. Error: {e}")

        del model, tokenizer
        gc.collect()
        torch.cuda.empty_cache()

# ==============================================================================
# STEP 5: AGGREGATE AND FORMAT TABLE 3
# ==============================================================================
print("\n--- Overall Analysis Complete! ---")
if table3_results:
    results_df = pd.DataFrame(table3_results)

    # Find the most frequent layer (mode) for biased sentences
    biased_mode = results_df.groupby(['dataset', 'model_name'])['biased_max_layer'].apply(lambda x: x.mode().iloc[0]).unstack(level=0)

    # Find the most frequent layer (mode) for neutral sentences
    neutral_mode = results_df.groupby(['dataset', 'model_name'])['neutral_max_layer'].apply(lambda x: x.mode().iloc[0]).unstack(level=0)

    # Combine the two results into one table
    final_table = pd.concat([biased_mode, neutral_mode], axis=1, keys=['Biased Sentences', 'Neutral Sentences'])

    # Reorder columns to match the paper
    final_table = final_table[['Biased Sentences', 'Neutral Sentences']].reindex(columns=['StereoSet', 'CrowS-Pairs'], level=1)

    print("\n\n--- Final Table (Replication of Table 3) ---")
    print(final_table)
else:
    print("\nNo results were generated. Please check for errors.")

# ==============================================================================
# STEP 1: INSTALL LIBRARIES
# ==============================================================================
print("--- Installing required libraries ---")
!pip install datasets transformers torch pandas numpy scipy accelerate -q


# ==============================================================================
# STEP 2: IMPORTS & SETUP
# ==============================================================================
import torch
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from scipy.spatial.distance import euclidean
import gc
from datasets import load_dataset
import matplotlib.pyplot as plt
import seaborn as sns

print("\n--- Initializing setup ---")

# --- Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- Lists of Models ---
MODELS_TO_RUN = [
    "albert-base-v2", "distilbert-base-uncased", "bert-base-uncased",
    "bert-large-uncased", "roberta-base", "roberta-large", "distilroberta-base",
    "openai-gpt", "gpt2"
]
DECODER_MODELS = ["openai-gpt", "gpt2"]
BERT_BASED_MODELS = [
    "albert-base-v2", "distilbert-base-uncased", "bert-base-uncased",
    "bert-large-uncased", "roberta-base", "roberta-large", "distilroberta-base"
]


# ==============================================================================
# STEP 3: DEFINE HELPER FUNCTIONS
# ==============================================================================
def find_distinct_words(sent1, sent2):
    words1 = set(str(sent1).lower().replace('.', '').replace(',', '').split())
    words2 = set(str(sent2).lower().replace('.', '').replace(',', '').split())
    distinct_words = list(words1.symmetric_difference(words2))
    word1 = next((w for w in distinct_words if w in words1), None)
    word2 = next((w for w in distinct_words if w in words2), None)
    return word1, word2

def get_model_outputs(sentence, target_word, model, tokenizer):
    layer_attentions, cls_embeddings = None, None
    try:
        inputs = tokenizer(str(sentence), return_tensors="pt", truncation=True, max_length=128).to(device)
        with torch.no_grad():
            outputs = model(**inputs, output_attentions=True, output_hidden_states=True)
        if hasattr(outputs, 'attentions') and outputs.attentions is not None and all(att is not None for att in outputs.attentions):
            tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
            word_indices = [i for i, token in enumerate(tokens) if target_word.lower() in token.lower()]
            if word_indices:
                target_index = word_indices[0]
                layer_attentions = [torch.sum(torch.mean(layer_attention, dim=1).squeeze(0)[:, target_index]).item() for layer_attention in outputs.attentions]
        if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
            cls_embeddings = [layer_hidden_state[0, 0, :].cpu().numpy() for layer_hidden_state in outputs.hidden_states[1:]]
    except Exception as e:
        print(f"    - Error during model processing: {e}")
        return None, None
    return layer_attentions, cls_embeddings


# ==============================================================================
# STEP 4: RUN ANALYSIS ACROSS BOTH DATASETS
# ==============================================================================
all_results = []
datasets_to_run = {
    "CrowS-Pairs": "crows_pairs_anonymized.csv",
    "StereoSet": "McGill-NLP/stereoset"
}

for dataset_name, data_source in datasets_to_run.items():
    print(f"\n{'='*20}\n--- Starting Analysis for: {dataset_name} ---\n{'='*20}")

    # --- Load and Prepare Data ---
    if dataset_name == "CrowS-Pairs":
        try:
            df = pd.read_csv(data_source)
            df = df.rename(columns={'sent_more': 'sentence_biased', 'sent_less': 'sentence_neutral'})
            print("CrowS-Pairs data loaded.")
        except FileNotFoundError:
            print(f"ERROR: Please upload '{data_source}' to run the analysis.")
            continue
    elif dataset_name == "StereoSet":
        ds = load_dataset(data_source, "intrasentence")['validation']
        sentence_pairs = []
        for example in ds:
            context, s_stereotype, s_anti = example['context'], None, None
            for i in range(len(example['sentences']['sentence'])):
                label, text = example['sentences']['gold_label'][i], example['sentences']['sentence'][i]
                if "BLANK" in context:
                    if label == 0: s_stereotype = context.split('BLANK')[0] + text + context.split('BLANK')[1]
                    elif label == 1: s_anti = context.split('BLANK')[0] + text + context.split('BLANK')[1]
            if s_stereotype and s_anti:
                sentence_pairs.append({'sentence_biased': s_stereotype, 'sentence_neutral': s_anti})
        df = pd.DataFrame(sentence_pairs)
        print("StereoSet data prepared.")

    # --- Run Models on the Loaded Data ---
    for model_name in MODELS_TO_RUN:
        print(f"\n--- Processing Model: {model_name} on {dataset_name} ---")
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token

            model_class = AutoModelForCausalLM if model_name in DECODER_MODELS else AutoModel
            model = model_class.from_pretrained(model_name, attn_implementation="eager").to(device)
            model.eval()

            # Using df.head(20) for a quicker demonstration. Remove .head(20) for the full run.
            for index, row in df.head(20).iterrows():
                word_b, word_n = find_distinct_words(row['sentence_biased'], row['sentence_neutral'])
                if not word_b or not word_n: continue

                att_b, cls_b = get_model_outputs(row['sentence_biased'], word_b, model, tokenizer)
                att_n, cls_n = get_model_outputs(row['sentence_neutral'], word_n, model, tokenizer)

                if att_b is None or att_n is None: continue

                result_dict = {'dataset': dataset_name, 'model_name': model_name}
                sad_scores = [abs(att_b[i] - att_n[i]) for i in range(len(att_b))]
                result_dict['max_sad'] = np.max(sad_scores)

                if model_name in BERT_BASED_MODELS and cls_b is not None and cls_n is not None:
                    result_dict['max_cls_dist_layer'] = np.argmax(cls_distances) + 1

                all_results.append(result_dict)
            print(f"  Finished processing {model_name}.")

        except Exception as e:
            print(f"Could not process model {model_name}. Error: {e}")

        del model, tokenizer
        gc.collect()
        torch.cuda.empty_cache()


# ==============================================================================
# STEP 5: DISPLAY FINAL COMPARATIVE RESULTS
# ==============================================================================
print("\n--- Overall Analysis Complete! ---")
if all_results:
    results_df = pd.DataFrame(all_results)

    print("\n--- Final Results Saved to final_results.csv ---")
    results_df.to_csv('final_results.csv', index=False)

    # --- SAD Results Table ---
    print("\n--- Mean SAD per Model (Comparison) ---")
    mean_sad_comparison = results_df.groupby(['dataset', 'model_name'])['max_sad'].mean().unstack()
    print(mean_sad_comparison)

    # --- Plotting the Comparison ---
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.figure(figsize=(15, 8))
    sns.barplot(
        data=results_df,
        x='model_name',
        y='max_sad',
        hue='dataset',
        palette='rocket',
        estimator=np.mean
    )
    plt.title('Comparison of Mean SAD Scores Across Datasets', fontsize=18)
    plt.xlabel('Model Name', fontsize=12)
    plt.ylabel('Mean of Max SAD Scores', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.legend(title='Dataset', fontsize=12)
    plt.tight_layout()
    plt.show()

else:
    print("\nNo results were generated. Please check for errors.")

# ==============================================================================
# STEP 1: INSTALL LIBRARIES
# ==============================================================================
print("--- Installing required libraries ---")
!pip install datasets transformers torch pandas numpy scipy accelerate -q


# ==============================================================================
# STEP 2: IMPORTS & SETUP
# ==============================================================================
import torch
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
import gc
from datasets import load_dataset
from IPython.display import display

print("\n--- Initializing setup ---")

# --- Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- Lists of Models ---
MODELS_TO_RUN = [
    "albert-base-v2", "distilbert-base-uncased", "bert-base-uncased",
    "bert-large-uncased", "roberta-base", "roberta-large", "distilroberta-base",
    "openai-gpt", "gpt2"
]
DECODER_MODELS = ["openai-gpt", "gpt2"]


# ==============================================================================
# STEP 3: DEFINE HELPER FUNCTIONS
# ==============================================================================
def find_distinct_words(sent1, sent2):
    words1 = set(str(sent1).lower().replace('.', '').replace(',', '').split())
    words2 = set(str(sent2).lower().replace('.', '').replace(',', '').split())
    distinct_words = list(words1.symmetric_difference(words2))
    word1 = next((w for w in distinct_words if w in words1), None)
    word2 = next((w for w in distinct_words if w in words2), None)
    return word1, word2

def get_attention_weights(sentence, target_word, model, tokenizer):
    layer_attentions = None
    try:
        inputs = tokenizer(str(sentence), return_tensors="pt", truncation=True, max_length=128).to(device)
        with torch.no_grad():
            outputs = model(**inputs, output_attentions=True)
        if hasattr(outputs, 'attentions') and outputs.attentions is not None and all(att is not None for att in outputs.attentions):
            tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
            word_indices = [i for i, token in enumerate(tokens) if target_word.lower() in token.lower()]
            if word_indices:
                target_index = word_indices[0]
                layer_attentions = [torch.sum(torch.mean(layer_attention, dim=1).squeeze(0)[:, target_index]).item() for layer_attention in outputs.attentions]
    except Exception as e:
        print(f"    - Error during model processing: {e}")
        return None
    return layer_attentions


# ==============================================================================
# STEP 4: RUN ANALYSIS ACROSS BOTH DATASETS
# ==============================================================================
all_results = []
datasets_to_run = {
    "CrowS-Pairs": "crows_pairs_anonymized.csv",
    "StereoSet": "McGill-NLP/stereoset"
}

for dataset_name, data_source in datasets_to_run.items():
    print(f"\n{'='*20}\n--- Starting Analysis for: {dataset_name} ---\n{'='*20}")

    # --- Load and Prepare Data ---
    if dataset_name == "CrowS-Pairs":
        try:
            df = pd.read_csv(data_source)
            df = df.rename(columns={'sent_more': 'sentence_biased', 'sent_less': 'sentence_neutral'})
        except FileNotFoundError:
            print(f"ERROR: Please upload '{data_source}' to run the analysis.")
            continue
    elif dataset_name == "StereoSet":
        ds = load_dataset(data_source, "intrasentence")['validation']
        sentence_pairs = []
        for example in ds:
            context, s_stereotype, s_anti = example['context'], None, None
            for i in range(len(example['sentences']['sentence'])):
                label, text = example['sentences']['gold_label'][i], example['sentences']['sentence'][i]
                if "BLANK" in context:
                    if label == 0: s_stereotype = context.split('BLANK')[0] + text + context.split('BLANK')[1]
                    elif label == 1: s_anti = context.split('BLANK')[0] + text + context.split('BLANK')[1]
            if s_stereotype and s_anti:
                sentence_pairs.append({'sentence_biased': s_stereotype, 'sentence_neutral': s_anti})
        df = pd.DataFrame(sentence_pairs)

    # --- Run Models on the Loaded Data ---
    for model_name in MODELS_TO_RUN:
        print(f"\n--- Processing Model: {model_name} on {dataset_name} ---")
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token

            model_class = AutoModelForCausalLM if model_name in DECODER_MODELS else AutoModel
            model = model_class.from_pretrained(model_name, attn_implementation="eager").to(device)
            model.eval()

            # Using df.head(30) for a quicker demonstration. Remove .head(30) for the full run.
            for index, row in df.head(30).iterrows():
                word_b, word_n = find_distinct_words(row['sentence_biased'], row['sentence_neutral'])
                if not word_b or not word_n: continue

                att_b = get_attention_weights(row['sentence_biased'], word_b, model, tokenizer)
                att_n = get_attention_weights(row['sentence_neutral'], word_n, model, tokenizer)

                if att_b is None or att_n is None: continue

                sad_scores = [abs(att_b[i] - att_n[i]) for i in range(len(att_b))]

                all_results.append({
                    'dataset': dataset_name,
                    'model_name': model_name,
                    'max_sad': np.max(sad_scores),
                    'max_sad_layer': np.argmax(sad_scores) + 1,
                })
            print(f"  Finished processing {model_name}.")

        except Exception as e:
            print(f"Could not process model {model_name}. Error: {e}")

        del model, tokenizer
        gc.collect()
        torch.cuda.empty_cache()

# ==============================================================================
# STEP 5: AGGREGATE AND DISPLAY TABLE 2
# ==============================================================================
print("\n--- Overall Analysis Complete! ---")
if all_results:
    results_df = pd.DataFrame(all_results)

    # --- Create a list to hold the summary for each dataset ---
    summary_dfs = []

    for dataset_name in datasets_to_run.keys():
        df_subset = results_df[results_df['dataset'] == dataset_name]

        if not df_subset.empty:
            # Calculate the three metrics for the current dataset
            sad_mean = df_subset.groupby('model_name')['max_sad'].mean()
            sad_max_value = df_subset.groupby('model_name')['max_sad'].max()
            layer_mode = df_subset.groupby('model_name')['max_sad_layer'].apply(lambda x: x.mode().iloc[0])

            # Combine into a single DataFrame
            summary = pd.DataFrame({
                'Layer SAD': layer_mode,
                'SAD (Max Value)': sad_max_value,
                'SAD-Mean': sad_mean
            }).astype({'Layer SAD': int})

            summary_dfs.append(summary)

    # Combine both summaries into the final table format
    if len(summary_dfs) == 2:
        final_table = pd.concat(summary_dfs, axis=1, keys=['CrowS-Pairs', 'StereoSet'])

        print("\n\n--- Final Table (Replication of Table 2) ---")
        display(final_table.style.format({
            ('CrowS-Pairs', 'SAD (Max Value)'): '{:.2f}',
            ('CrowS-Pairs', 'SAD-Mean'): '{:.2f}',
            ('StereoSet', 'SAD (Max Value)'): '{:.2f}',
            ('StereoSet', 'SAD-Mean'): '{:.2f}'
        }))
    else:
        print("Could not generate the final table because results for one or both datasets are missing.")

else:
    print("\nNo results were generated. Please check for errors.")

# ==============================================================================
# STEP 1: INSTALL LIBRARIES
# ==============================================================================
print("--- Installing required libraries ---")
!pip install datasets transformers torch pandas numpy scipy accelerate -q


# ==============================================================================
# STEP 2: IMPORTS & SETUP
# ==============================================================================
import torch
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModel
from scipy.spatial.distance import euclidean
import gc
from datasets import load_dataset
from IPython.display import display

print("\n--- Initializing setup ---")

# --- Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# We only need the BERT-based models for this table
BERT_BASED_MODELS = [
    "albert-base-v2", "distilbert-base-uncased", "bert-base-uncased",
    "bert-large-uncased", "roberta-base", "roberta-large", "distilroberta-base"
]


# ==============================================================================
# STEP 3: DEFINE HELPER FUNCTIONS
# ==============================================================================
def get_cls_embeddings(sentence, model, tokenizer):
    cls_embeddings = None
    try:
        inputs = tokenizer(str(sentence), return_tensors="pt", truncation=True, max_length=128).to(device)
        with torch.no_grad():
            outputs = model(**inputs, output_hidden_states=True)
        if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
            cls_embeddings = [layer[0, 0, :].cpu().numpy() for layer in outputs.hidden_states[1:]]
    except Exception as e:
        print(f"    - Error getting CLS embeddings: {e}")
    return cls_embeddings


# ==============================================================================
# STEP 4: RUN ANALYSIS FOR TABLE 1
# ==============================================================================
cls_results = []
datasets_to_run = {
    "CrowS-Pairs": "crows_pairs_anonymized.csv",
    "StereoSet": "McGill-NLP/stereoset"
}

for dataset_name, data_source in datasets_to_run.items():
    print(f"\n{'='*20}\n--- Starting Analysis for: {dataset_name} ---\n{'='*20}")

    # Load and Prepare Data
    if dataset_name == "CrowS-Pairs":
        try:
            df = pd.read_csv(data_source).rename(columns={'sent_more': 'sentence_biased', 'sent_less': 'sentence_neutral'})
        except FileNotFoundError:
            print(f"ERROR: Please upload '{data_source}'.")
            continue
    elif dataset_name == "StereoSet":
        ds = load_dataset(data_source, "intrasentence")['validation']
        sentence_pairs = []
        for example in ds:
            context, s_stereotype, s_anti = example['context'], None, None
            for i in range(len(example['sentences']['sentence'])):
                label, text = example['sentences']['gold_label'][i], example['sentences']['sentence'][i]
                if "BLANK" in context:
                    if label == 0: s_stereotype = context.split('BLANK')[0] + text + context.split('BLANK')[1]
                    elif label == 1: s_anti = context.split('BLANK')[0] + text + context.split('BLANK')[1]
            if s_stereotype and s_anti:
                sentence_pairs.append({'sentence_biased': s_stereotype, 'sentence_neutral': s_anti})
        df = pd.DataFrame(sentence_pairs)

    # Run Models on the Loaded Data
    for model_name in BERT_BASED_MODELS:
        print(f"\n--- Processing Model: {model_name} on {dataset_name} ---")
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModel.from_pretrained(model_name, attn_implementation="eager").to(device)
            model.eval()

            for index, row in df.head(30).iterrows(): # Using a sample for speed. Remove .head(30) for full analysis.
                cls_b = get_cls_embeddings(row['sentence_biased'], model, tokenizer)
                cls_n = get_cls_embeddings(row['sentence_neutral'], model, tokenizer)

                if cls_b is not None and cls_n is not None:
                    cls_distances = [euclidean(cls_b[i], cls_n[i]) for i in range(len(cls_b))]
                    cls_results.append({
                        'dataset': dataset_name,
                        'model_name': model_name,
                        'max_cls_dist_layer': np.argmax(cls_distances) + 1
                    })
            print(f"  Finished processing {model_name}.")

        except Exception as e:
            print(f"Could not process model {model_name}. Error: {e}")

        del model, tokenizer
        gc.collect()
        torch.cuda.empty_cache()

# ==============================================================================
# STEP 5: AGGREGATE AND DISPLAY TABLE 1
# ==============================================================================
print("\n--- Overall Analysis Complete! ---")
if cls_results:
    results_df = pd.DataFrame(cls_results)

    # For each model and dataset, find the most frequent (mode) layer
    cls_layer_mode = results_df.groupby(['dataset', 'model_name'])['max_cls_dist_layer'].apply(lambda x: x.mode().iloc[0])

    # Pivot the table so datasets become columns
    final_table = cls_layer_mode.unstack(level=0)

    # Rename and format to match the paper
    final_table = final_table.rename(columns={'CrowS-Pairs': 'Crows-Pairs', 'StereoSet': 'Stereoset'})
    final_table = final_table.astype(int)

    print("\n\n--- Final Table (Replication of Table 1) ---")
    display(final_table)
else:
    print("\nNo CLS distance results were generated.")

# ==============================================================================
# STEP 1: INSTALL AND IMPORT LIBRARIES
# ==============================================================================
print("--- Installing required libraries ---")
!pip install datasets pandas -q

from datasets import load_dataset
import pandas as pd
import re

print("Libraries installed and imported.")


# ==============================================================================
# STEP 2: LOAD A SAMPLE OF THE TRAINING DATA (CORRECTED)
# ==============================================================================
print("\n--- Loading a sample of the C4 dataset ---")
# 'streaming=True' avoids downloading the entire massive dataset.
# We will take the first 10,000 text snippets for a reasonably fast analysis.
SAMPLE_SIZE = 10000

# --- THIS IS THE CORRECTED LINE ---
# We are now using the 'c4' dataset instead of the deprecated 'wikipedia' one.
data_stream = load_dataset("allenai/c4", "en", split='train', streaming=True)

text_sample = []
for i, example in enumerate(data_stream):
    if i >= SAMPLE_SIZE:
        break
    text_sample.append(example['text'])

print(f"Loaded a sample of {len(text_sample)} text snippets from C4.")


# ==============================================================================
# STEP 3: DEFINE STEREOTYPICAL WORD PAIRS TO INVESTIGATE
# ==============================================================================
word_pairs_to_test = [
    ('man', 'engineer', 'Gender-Profession'),
    ('woman', 'engineer', 'Gender-Profession'),
    ('woman', 'nurse', 'Gender-Profession'),
    ('man', 'nurse', 'Gender-Profession'),
    ('woman', 'emotional', 'Gender-Attribute'),
    ('man', 'rational', 'Gender-Attribute'),
    ('black', 'criminal', 'Race-Attribute'),
    ('white', 'criminal', 'Race-Attribute')
]
print("\n--- Defined word pairs for analysis ---")


# ==============================================================================
# STEP 4: CREATE THE CO-OCCURRENCE COUNTING FUNCTION
# ==============================================================================
def count_cooccurrence(text_corpus, word1, word2):
    """Counts how many sentences in the corpus contain both word1 and word2."""
    count = 0
    pattern1 = r'\b' + re.escape(word1) + r'\b'
    pattern2 = r'\b' + re.escape(word2) + r'\b'

    for text_block in text_corpus:
        sentences = text_block.split('.')
        for sentence in sentences:
            if re.search(pattern1, sentence, re.IGNORECASE) and re.search(pattern2, sentence, re.IGNORECASE):
                count += 1
    return count

print("Analysis function is ready.")


# ==============================================================================
# STEP 5: RUN THE ANALYSIS AND DISPLAY RESULTS
# ==============================================================================
print("\n--- Running co-occurrence analysis (this may take a few minutes) ---")
analysis_results = []

for word1, word2, description in word_pairs_to_test:
    count = count_cooccurrence(text_sample, word1, word2)
    analysis_results.append({
        'Pair': f"({word1}, {word2})",
        'Description': description,
        'Co-occurrence Count': count
    })
    print(f"Count for ({word1}, {word2}): {count}")

# Create a DataFrame for a clean display
results_df = pd.DataFrame(analysis_results)

print("\n\n--- Qualitative Analysis Results ---")
display(results_df)

# ==============================================================================
# STEP 1: INSTALL LIBRARIES
# ==============================================================================
print("--- Installing required libraries ---")
!pip install datasets transformers torch pandas numpy scipy accelerate -q


# ==============================================================================
# STEP 2: IMPORTS & SETUP
# ==============================================================================
import torch
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from scipy.spatial.distance import euclidean
import gc
from datasets import load_dataset
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display

print("\n--- Initializing setup ---")

# --- Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- Lists of Models ---
MODELS_TO_RUN = [
    "albert-base-v2", "distilbert-base-uncased", "bert-base-uncased",
    "bert-large-uncased", "roberta-base", "roberta-large", "distilroberta-base",
    "openai-gpt", "gpt2"
]
DECODER_MODELS = ["openai-gpt", "gpt2"]
BERT_BASED_MODELS = [
    "albert-base-v2", "distilbert-base-uncased", "bert-base-uncased",
    "bert-large-uncased", "roberta-base", "roberta-large", "distilroberta-base"
]


# ==============================================================================
# STEP 3: DEFINE HELPER FUNCTIONS
# ==============================================================================
def find_distinct_words(sent1, sent2):
    words1 = set(str(sent1).lower().replace('.', '').replace(',', '').split())
    words2 = set(str(sent2).lower().replace('.', '').replace(',', '').split())
    distinct_words = list(words1.symmetric_difference(words2))
    word1 = next((w for w in distinct_words if w in words1), None)
    word2 = next((w for w in distinct_words if w in words2), None)
    return word1, word2

def get_model_outputs(sentence, target_word, model, tokenizer):
    layer_attentions, cls_embeddings = None, None
    try:
        inputs = tokenizer(str(sentence), return_tensors="pt", truncation=True, max_length=128).to(device)
        with torch.no_grad():
            outputs = model(**inputs, output_attentions=True, output_hidden_states=True)
        if hasattr(outputs, 'attentions') and outputs.attentions is not None and all(att is not None for att in outputs.attentions):
            tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
            word_indices = [i for i, token in enumerate(tokens) if target_word.lower() in token.lower()]
            if word_indices:
                target_index = word_indices[0]
                layer_attentions = [torch.sum(torch.mean(layer_attention, dim=1).squeeze(0)[:, target_index]).item() for layer_attention in outputs.attentions]
        if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
            cls_embeddings = [layer[0, 0, :].cpu().numpy() for layer in outputs.hidden_states[1:]]
    except Exception as e:
        print(f"    - Error during model processing: {e}")
        return None, None
    return layer_attentions, cls_embeddings


# ==============================================================================
# STEP 4: RUN FULL ANALYSIS ACROSS BOTH DATASETS
# ==============================================================================
all_results = []
datasets_to_run = {
    "CrowS-Pairs": "crows_pairs_anonymized.csv",
    "StereoSet": "McGill-NLP/stereoset"
}

for dataset_name, data_source in datasets_to_run.items():
    print(f"\n{'='*20}\n--- Starting Analysis for: {dataset_name} ---\n{'='*20}")

    # --- Load and Prepare Data ---
    if dataset_name == "CrowS-Pairs":
        try:
            df = pd.read_csv(data_source).rename(columns={'sent_more': 'sentence_biased', 'sent_less': 'sentence_neutral'})
        except FileNotFoundError:
            print(f"ERROR: Please upload '{data_source}' to run the analysis.")
            continue
    elif dataset_name == "StereoSet":
        ds = load_dataset(data_source, "intrasentence")['validation']
        sentence_pairs = []
        for example in ds:
            context, s_stereotype, s_anti = example['context'], None, None
            for i in range(len(example['sentences']['sentence'])):
                label, text = example['sentences']['gold_label'][i], example['sentences']['sentence'][i]
                if "BLANK" in context:
                    if label == 0: s_stereotype = context.split('BLANK')[0] + text + context.split('BLANK')[1]
                    elif label == 1: s_anti = context.split('BLANK')[0] + text + context.split('BLANK')[1]
            if s_stereotype and s_anti:
                sentence_pairs.append({'sentence_biased': s_stereotype, 'sentence_neutral': s_anti})
        df = pd.DataFrame(sentence_pairs)

    # --- Run Models on the Loaded Data ---
    for model_name in MODELS_TO_RUN:
        print(f"\n--- Processing Model: {model_name} on {dataset_name} ---")
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token

            model_class = AutoModelForCausalLM if model_name in DECODER_MODELS else AutoModel
            model = model_class.from_pretrained(model_name, attn_implementation="eager").to(device)
            model.eval()

            # Using df.head(30) for a quicker demonstration. REMOVE .head(30) for the full, final run.
            for index, row in df.head(30).iterrows():
                word_b, word_n = find_distinct_words(row['sentence_biased'], row['sentence_neutral'])
                if not word_b or not word_n: continue

                att_b, cls_b = get_model_outputs(row['sentence_biased'], word_b, model, tokenizer)
                att_n, cls_n = get_model_outputs(row['sentence_neutral'], word_n, model, tokenizer)

                if att_b is None or att_n is None: continue

                result_dict = {'dataset': dataset_name, 'model_name': model_name}
                sad_scores = [abs(att_b[i] - att_n[i]) for i in range(len(att_b))]
                result_dict['max_sad'] = np.max(sad_scores)
                result_dict['max_sad_layer'] = np.argmax(sad_scores) + 1
                result_dict['biased_max_layer'] = np.argmax(att_b) + 1
                result_dict['neutral_max_layer'] = np.argmax(att_n) + 1

                if model_name in BERT_BASED_MODELS and cls_b is not None and cls_n is not None:
                    cls_distances = [euclidean(cls_b[i], cls_n[i]) for i in range(len(cls_b))]
                    result_dict['max_cls_dist_layer'] = np.argmax(cls_distances) + 1

                all_results.append(result_dict)
            print(f"  Finished processing {model_name}.")

        except Exception as e:
            print(f"Could not process model {model_name}. Error: {e}")

        del model, tokenizer
        gc.collect()
        torch.cuda.empty_cache()

# ==============================================================================
# STEP 5: DISPLAY FINAL TABLES AND PLOT (CORRECTED)
# ==============================================================================
print("\n--- Overall Analysis Complete! ---")
if all_results:
    results_df = pd.DataFrame(all_results)

    # --- GENERATE TABLE 1 (CLS Layers) ---
    print("\n\n--- Final Table (Replication of Table 1) ---")
    cls_results = results_df.dropna(subset=['max_cls_dist_layer'])
    if not cls_results.empty:
        table1 = cls_results.groupby(['dataset', 'model_name'])['max_cls_dist_layer'].apply(lambda x: x.mode().iloc[0]).unstack(level=0)
        table1 = table1.rename(columns={'CrowS-Pairs': 'Crows-Pairs', 'StereoSet': 'Stereoset'}).astype(int)
        display(table1)
    else:
        print("No CLS distance results were generated for Table 1.")

    # --- GENERATE TABLE 2 (SAD Scores) - CORRECTED ---
    print("\n\n--- Final Table (Replication of Table 2) ---")
    summary_dfs = []
    for name, group in results_df.groupby('dataset'):
        sad_mean = group.groupby('model_name')['max_sad'].mean()
        sad_max_value = group.groupby('model_name')['max_sad'].max()
        layer_mode = group.groupby('model_name')['max_sad_layer'].apply(lambda x: x.mode().iloc[0])
        summary = pd.DataFrame({'Layer SAD': layer_mode, 'SAD (Max Value)': sad_max_value, 'SAD-Mean': sad_mean})
        summary_dfs.append(summary)

    table2 = pd.concat(summary_dfs, axis=1, keys=['CrowS-Pairs', 'StereoSet'])
    # Define formatters for the specific columns that need formatting
    formatters = {
        ('CrowS-Pairs', 'SAD (Max Value)'): '{:.2f}', ('CrowS-Pairs', 'SAD-Mean'): '{:.2f}',
        ('StereoSet', 'SAD (Max Value)'): '{:.2f}', ('StereoSet', 'SAD-Mean'): '{:.2f}'
    }
    display(table2.style.format(formatters))

    # --- GENERATE TABLE 3 (Highest Attention Layers) ---
    print("\n\n--- Final Table (Replication of Table 3) ---")
    biased_mode = results_df.groupby(['dataset', 'model_name'])['biased_max_layer'].apply(lambda x: x.mode().iloc[0]).unstack(level=0)
    neutral_mode = results_df.groupby(['dataset', 'model_name'])['neutral_max_layer'].apply(lambda x: x.mode().iloc[0]).unstack(level=0)
    table3 = pd.concat([biased_mode, neutral_mode], axis=1, keys=['Biased Sentences', 'Neutral Sentences'])
    table3 = table3.reindex(columns=['StereoSet', 'CrowS-Pairs'], level=1).astype(int)
    display(table3)

    # --- GENERATE COMPARATIVE PLOT ---
    print("\n\n--- Final Plot: Comparison of Mean SAD Scores ---")
    plt.figure(figsize=(15, 8))
    sns.barplot(data=results_df, x='model_name', y='max_sad', hue='dataset', palette='rocket', estimator=np.mean)
    plt.title('Comparison of Mean SAD Scores Across Datasets', fontsize=18)
    plt.xlabel('Model Name', fontsize=12)
    plt.ylabel('Mean of Max SAD Scores', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.legend(title='Dataset', fontsize=12)
    plt.tight_layout()
    plt.show()

else:
    print("\nNo results were generated. Please check for errors.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ==============================================================================
# STEP 1: PREPARE THE DATA
# ==============================================================================
# This assumes 'results_df' was successfully populated by the main analysis script.
if 'results_df' in locals() and not results_df.empty:

    # Filter for the results that have CLS distance data (i.e., BERT-based models)
    cls_results_df = results_df.dropna(subset=['max_cls_dist', 'max_cls_dist_layer'])

    if not cls_results_df.empty:

        # ==============================================================================
        # STEP 2: DEFINE COHEN'S D FUNCTION
        # ==============================================================================
        def calculate_cohen_d(group1, group2):
            """Calculates Cohen's d for independent samples."""
            n1, n2 = len(group1), len(group2)
            if n1 < 2 or n2 < 2: return 0.0 # Cannot compute with single-item groups
            mean1, mean2 = np.mean(group1), np.mean(group2)
            std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)

            pooled_std = np.sqrt(((n1 - 1) * std1 ** 2 + (n2 - 1) * std2 ** 2) / (n1 + n2 - 2))
            if pooled_std == 0: return 0.0 # Avoid division by zero

            d = (mean1 - mean2) / pooled_std
            return abs(d)

        # ==============================================================================
        # STEP 3: LOOP THROUGH EACH DATASET TO CREATE A HEATMAP
        # ==============================================================================
        for dataset_name in ['CrowS-Pairs', 'StereoSet']:

            print(f"\n--- Generating Heatmap for: {dataset_name} ---")

            # Filter the data for the current dataset
            dataset_df = cls_results_df[cls_results_df['dataset'] == dataset_name]

            if dataset_df.empty:
                print(f"No CLS data found for {dataset_name}. Skipping heatmap.")
                continue

            model_names = dataset_df['model_name'].unique()
            cohen_d_matrix = pd.DataFrame(index=model_names, columns=model_names, dtype=float)

            # Loop through each pair of models to fill the matrix
            for model1 in model_names:
                for model2 in model_names:
                    dist1 = dataset_df[dataset_df['model_name'] == model1]['max_cls_dist']
                    dist2 = dataset_df[dataset_df['model_name'] == model2]['max_cls_dist']

                    d_value = calculate_cohen_d(dist1, dist2)
                    cohen_d_matrix.loc[model1, model2] = d_value

            # --- Generate the Heatmap ---
            plt.figure(figsize=(10, 8))
            sns.heatmap(
                cohen_d_matrix,
                annot=True,     # Show the numbers in the cells
                cmap='Blues',   # Use a blue color scheme
                fmt=".2f"       # Format numbers to two decimal places
            )
            plt.title(f"Heatmap of CLS Distances for {dataset_name} (Cohen's d)", fontsize=16)
            plt.show()

    else:
        print("No CLS distance data was found to generate the heatmaps.")

else:
    print("Error: The 'results_df' DataFrame was not found.")
    print("Please make sure you have successfully run the full, all-in-one analysis script.")

